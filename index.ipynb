{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders Lab\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this lab, we'll learn how to write an **_encoder_** and a **_decoder_**, and stack them into an **_Autoencoder_**. \n",
    "\n",
    "## The Data Set\n",
    "\n",
    "For this lab, we'll be working with the MNIST dataset.  \n",
    "\n",
    "## Getting Started: What are Autoencoders?\n",
    "\n",
    "Autoencoders are a clever way of training two models together than can compress an image (or other data) down into a roughly equivalent representation in a lower-dimensional space (the **_Encoder_**), and then reconstruct the original image from that lower-dimensional representation.  At their heart, they're a compression technique.  However, they're not a _good_ compression technique, because they are **_lossy_**, meaning that some of the compressed information will be lost during the process--in the case of images, we can interpret this as the image being a bit lower quality after running through the autoencoder.  \n",
    "\n",
    "Although autoencoders are not great at compression, they're still a useful tool in our Deep Learning toolbox, as they introduce us to the concept of **_Data Manifolds_**.  Although basic autoencoders aren't too useful, it turns out that more advanced versions of autoencoders have amazing uses for **_generative techniques_**, such as **_Denoising Autoencoders_** and **_Variational Autoencoders_**. Before we can learn to use the advanced methods, we'll need to get some practice and mastery with basic autoencoders.  \n",
    "\n",
    "Run the cell below to import everything we'll need for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(0)\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, Dense, Flatten, Input, MaxPooling2D, UpSampling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our First Autoencoder\n",
    "\n",
    "An Autoencoder is made up of 2 parts--an **_Encoder_**, and a **_Decoder_**.  These two parts are mirror images of each other.\n",
    "\n",
    "The **_encoder_** takes in some data, and outputs a representation of this data in a **_Z vector_**, which is the (generally equivalent) representation of the data in a lower dimensional space.  \n",
    "\n",
    "the **_decoder_** takes in a Z vector, and outputs the original image the encoder was given.  It won't be _exactly_ perfect, since these models are lossy and lose some information during the compression stage, but if the model is well trained, it'll generally be pretty close.  \n",
    "\n",
    "<img src='autoencoder_diagram.png'>\n",
    "\n",
    "**_Note:_** In this lab, we're using a kind of Keras syntax that you probably haven't seen in labs thus far.  Instead of creating a `Sequential()` object and using `.add()` to add each successive layer that we want, we instead instantiate the individual layers that we want, and specify what that given layer is connected to.  Then, we create a `Model` object and specify the input and output layers.\n",
    "\n",
    "To give you an example of this syntax so that you can get comfortable with it, we've provided the sample code for the first autoencoder in this lesson.  \n",
    "\n",
    "Before we can build the autoencoder, we'll need to decide how many dimensions we want our z-vector to be, which will determine the dimensionality of our encoded data.  \n",
    "\n",
    "In the cell below, set `encoding_dim` to 32. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, take a minute to read the code in the cell below and get a feel for the syntax used for constructing this autoencoder.  Then, run the cell to create a basic autoencoder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(784,))\n",
    "encoded_image_layer = Dense(encoding_dim, activation='relu')(input_img)\n",
    "decoded_image_layer = Dense(784, activation='sigmoid')(encoded_image_layer)\n",
    "autoencoder = Model(input_img, decoded_image_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the full autoencoder, we need to store the encoder and the decoder separately, so that we can use them one at a time once the model is trained.  We can do this by creating new Model objects, but referencing the layers from the model we created above. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Create a `Model` and store it in the variable `encoder`.  The input for this model is `input_img`, and the output for this model is `encoded_image_layer`.  \n",
    "* Create an `Input` layer and store it in `encoded_input`.  Set the `shape` parameter of this input layer to `(encoding_dim,)`\n",
    "* Slice the last layer from our autoencoder's `.layers` attribute and store it in `decoder_layer`\n",
    "* Create another `Model` object and store it in `decoder`.  The input of this model should be `encoded_input`, and the ouput should be `decoder_layer(encoded_input)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = None\n",
    "encoded_input = None\n",
    "decoder_layer = None\n",
    "decoder = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we've built our autoencoder and created separate encoder and decoder variables, we still need to compile our autoencoder model.  \n",
    "\n",
    "In the cell below, `.compile()` our autoencoder.  Set the loss to `'binary_crossentropy'` and the optimizer to `'adadelta'`.  \n",
    "\n",
    "(Unlike all the models from previous labs, we won't ask for any `metrics`, since it doesn't make sense for an autoencoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting our Training Data\n",
    "\n",
    "We'll train our autoencoder on MNIST for this lab.  However, we do not need the labels, just the images.  \n",
    "\n",
    "In the cell below, call `mnist.load_data()` to store our images in the appropriate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, _), (X_test, _) = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll still need to reshape our data into vectors and normalize it.  In the cell below:\n",
    "\n",
    "* Reshape the data inside `X_train` and `X_test` into vectors of size `784` (remember to include the number of samples in each as the first parameter in the `reshape` call.) Also cast the data in each to type `'float32'`.\n",
    "* Normalize the values by dividing the data in each by `255.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = None\n",
    "X_test = None\n",
    "X_train None\n",
    "X_test None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a compiled model and our data is reshaped and normalized.  All that is left is to fit the model!\n",
    "\n",
    "In the cell below, `.fit()` the autoencoder.  \n",
    "\n",
    "* Pass in `X_train` as the first and second arguments, since the images will act as both our data and our labels.\n",
    "* Train the model for `50` epochs.\n",
    "* Set a batch size of `256`.\n",
    "* Set the `shuffle` parameter to `True`.\n",
    "* Pass in `(X_test, X_test)` as our `validation_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained our autoencoder, let's make use the the encoder and decoder separately, and then visualize our results!\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Use the `encoder` object to create an encoded representation of `X_test` (use the `.predict()` method!)\n",
    "* Use the `decoder` object to decode the `encoded_imgs` we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs = None\n",
    "decoded_imgs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, runt he cell below to visualize a a comparison of original images and their encoded/decoded counterparts.  Do you detect much degradation in the images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization code borrowed from Keras Blog's article on Autoencoders\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(X_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Deeper Autoencoder\n",
    "\n",
    "The model we just built was the simplest possible version of an autoencoder.  It did pretty well considering how small it is, but it still definitely lost some information and degraded the images during the process.  Let's build a deeper model and see how this affects our image degradation!\n",
    "\n",
    "In the cell below, complete the following layers:\n",
    "\n",
    "* `input_img` should be an `Input` layer of shape `(784,)`\n",
    "* `encoded_1` should be a Dense layer with 128 neurons, relu activation, and should be connected to `input_img`\n",
    "* `encoded_2` should be a Dense layer with 64 neurons, relu activation, and should be connected to `encoded_1`\n",
    "* `encoded_3` should be a Dense layer with 32 neurons, relu activation, and should be connected to `encoded_2`\n",
    "* `decoded_1` should be a Dense layer with 64 neurons, relu activation, and should be connected to `encoded_3`\n",
    "* `decoded_2` should be a Dense layer with 128 neurons, relu activation, and should be connected to `decoded_1`\n",
    "* `decoded_3` should be a Dense layer with 784 neurons, sigmoid activation, and should be connected to `decoded_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = None\n",
    "encoded_1 = None\n",
    "encoded_2 = None\n",
    "encoded_3 = None\n",
    "\n",
    "decoded_1 = None\n",
    "decoded_2 = None\n",
    "decoded_3 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a `Model` object and pass in the first and last layers: `input_img` and `decoded_3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeper_autoencoder = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created our Deep Autoencoder, we'll still need to store references to our Deep Encoder and Deep Decoder separately, so that we can use them individually when needed.  \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Store a `Model()` in `deep_encoder`.  In this model, pass in our `input_img` and the last encoder layer  of our deep autoencoder, `encoder_3`.\n",
    "* Store an `Input` layer inside of `deep_encoded_input`, and set the `shape` parameter equal to `(encoding_dim,)`\n",
    "* Store the last layer from `deep_autoencoder` inside of `deep_decoder_layer` (if you're unsure how to do this, look at how we sliced from the `layers` attribute with the previous autoencoder)\n",
    "* Store another `Model()` unit, this time inside of `deep_decoder`.  Pass in `deep_encoded_input` and `decoder_layer(deep_encoded_input)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_encoder = None\n",
    "deep_encoded_input = None\n",
    "deep_decoder_layeNoner = None\n",
    "deep_decoder = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compile the model with the same parameters we did before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's fit our Deep Autoencoder. When you fit it this time, set the following parameters:\n",
    "* `epochs=100`\n",
    "* `batch_size=256`\n",
    "* `shuffle=True`\n",
    "* `validation_data=(X_test, X_test)`\n",
    "\n",
    "Don't forget to pass in `X_train` as the first two positional arguments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's encode and then decode the data in `X_test` with our Deep Autonencoder, so that we can reuse our visualization code and compare our decoded images with the original images.\n",
    "\n",
    "In the cell below, encode and then decode the images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_encoded_imgs = None\n",
    "deep_decoded_imgs  = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cell below to visualize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(X_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There still seems to be a little information loss in the form of image degradation, but the results are clearly better than the simple encoder we built before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Convolutional Autoencoders (Optional, GPU recommended)\n",
    "\n",
    "The best autoencoders make use of Convolutional layers.  We've included the following code to build a Deep Convolutional Autoencoder. However, the training time on this model will be quite long.  \n",
    "\n",
    "We've provided the example code to build the model.  However, we haven't provided the code for storing the encoder and decoder portions separately, or the code to compile and fit the model.  If you're up for a challenge, and don't mind a long run time, try to get the Deep Convolutional Autoencoder working below!\n",
    "\n",
    "### The Architecture\n",
    "\n",
    "If you read the code of the model below, you'll probably notice that the encoder portion looks fairly straightforward--the architecture is in line with what you would expect from a traditional CNN model--Conv2d layers with relu activations, followed by MaxPooling layers to reduce dimensionality.  However, in the decoder, you may have noticed a layer type we haven't encountered before--an **_UpSampling2D_** layer! This isn't as confusing as it looks--an upsampling layer is essentially the opposite of a MaxPooling layer. Whereas MaxPooling reduces our dimensionality section by section using a filter, Upsampling increases it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_input_img = Input(shape=(28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "conv_encoder_1 = Conv2D(16, (3, 3), activation='relu', padding='same')(conv_input_img)\n",
    "conv_encoder_2 = MaxPooling2D((2, 2), padding='same')(conv_encoder_1)\n",
    "conv_encoder_3 = Conv2D(8, (3, 3), activation='relu', padding='same')(conv_encoder_2)\n",
    "conv_encoder_4 = MaxPooling2D((2, 2), padding='same')(conv_encoder_3)\n",
    "conv_encoder_5 = Conv2D(8, (3, 3), activation='relu', padding='same')(conv_encoder_4)\n",
    "conv_encoded = MaxPooling2D((2, 2), padding='same')(conv_encoder_5)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "conv_decoder_1 = Conv2D(8, (3, 3), activation='relu', padding='same')(conv_encoded)\n",
    "conv_decoder_2 = UpSampling2D((2, 2))(conv_decoder_1)\n",
    "conv_decoder_3 = Conv2D(8, (3, 3), activation='relu', padding='same')(conv_decoder_2)\n",
    "conv_decoder_4 = UpSampling2D((2, 2))(conv_decoder_3)\n",
    "conv_decoder_5 = Conv2D(16, (3, 3), activation='relu')(conv_decoder_4)\n",
    "conv_decoder_6 = UpSampling2D((2, 2))(conv_decoder_5)\n",
    "conv_decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(conv_decoder_6)\n",
    "\n",
    "conv_autoencoder = Model(conv_input_img, conv_decoded)\n",
    "conv_autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "Recall that we reshaped our data to a vector since we were working with Dense layers in the previous models.  This won't work for a Convolutional Autoencoder, since the `Conv2D` layers will expect 2 dimensional images as inputs. \n",
    "\n",
    "The easiest option for us here is to just reimport the data and store it separately.\n",
    "\n",
    "In the cell below, import the data using the `mnist` module we imported above.  Normalize the data by dividing by `255.` (you'll need to use numpy for this), and reshape it from it's current shape to `(28, 28, 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, _), (X_test, _) = None\n",
    "X_train = None\n",
    "X_test = None\n",
    "X_train = None\n",
    "X_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Remaining Steps\n",
    "\n",
    "In order to complete this optional section, you'll still need to:\n",
    "\n",
    "* Store the encoder and the decoder in separate variables.\n",
    "* Compile and train the model.\n",
    "* Encode and then decode the images in `X_test`\n",
    "* Reuse our visualization code to compare the output with the original images!\n",
    "\n",
    "**_NOTE:_** You've been warned--this will likely take a very long time to train on a CPU.  If you don't have a GPU to train on, this will take a while!\n",
    "\n",
    "Try to complete the steps mentioned above and train your very own Deep Convolutional Autoencoder!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "[Keras Blog--Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
